# Tiny DLLM config (~25M params)
# Good for testing on any GPU or even CPU

model:
  d_model: 256
  n_heads: 4
  n_layers: 4
  d_ff: 1024
  dropout: 0.1

data:
  dataset: "wikitext"
  subset: "wikitext-2-raw-v1"
  text_column: "text"
  max_length: 256
  batch_size: 16
  num_workers: 4

training:
  total_steps: 10000
  lr: 3e-4
  weight_decay: 0.01
  warmup_steps: 500
  gradient_accumulation_steps: 1
  use_amp: true
  log_interval: 100
  save_interval: 2000
  eval_interval: 500

diffusion:
  schedule: "cosine"

tokenizer: "gpt2"
output_dir: "checkpoints/tiny"
